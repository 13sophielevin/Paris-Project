{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A routine to fetch pdfs\n",
    "\n",
    "Author, title, and year are \"and'd\" in the query.  If you need to \"or\", call this routine once for each part of the or.\n",
    "\n",
    "If you want data for only one year, provide a starting_year and ending_year with the same value.\n",
    "\n",
    "The Gallica CQL processor doesn't seem to handle dates like you'd expect given the documentation available elsewhere.  Part of the problem is that their dc:date field is a free-text field.  I'm not sure what the other part of the problem is, since their API doesn't provide error messages for CQL errors; instead, it simply fails.\n",
    "\n",
    "This routine figures out what years are between starting_year and ending_year, if starting_year and ending_year are supplied.  For each of those years (or just once, if no year is supplied)\n",
    "\n",
    "1.  It searches for that year using the author and title passed to the routine;\n",
    "2.  For each record it finds, it looks up the pagination;\n",
    "3.  It uses the pagination to build the URL for the PDF (Gallica's doc doesn't say how to do this), downloads the pdf, and renames it.\n",
    "\n",
    "The script is supposed to handle instances where the search results extend past more that one page of results,  And I think it actually may (I inadvertently tested it, but not comprehensively).  I have not tested the date range business.\n",
    "\n",
    "The process seems quite slow.  It took me about 12 minutes to download 21 pdf's . . . \n",
    "\n",
    "Also, please note that I'm not using PyGallica because it doesn't really do anything useful.  It's search API simply dumps the XML results to the file system, and does nothing to simplify the composition of CQL queries, and it's document API doesn't do anything for PDF's.  It really is quite useless . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, subprocess, re\n",
    "from lxml import etree\n",
    "\n",
    "ns = {'srw': 'http://www.loc.gov/zing/srw/', \n",
    "      'dc': 'http://purl.org/dc/elements/1.1/'}\n",
    "\n",
    "def get_pdfs_for_query(author=None, \n",
    "                     title=None, \n",
    "                     starting_year=None,\n",
    "                     ending_year=None,\n",
    "                     max_n_pdfs_to_fetch=100):\n",
    "    \n",
    "    starting_record = 1\n",
    "    \n",
    "    years = []\n",
    "    if starting_year != None and ending_year != None:\n",
    "        for year in range(int(starting_year), int(ending_year) + 1):\n",
    "            years.append(year)\n",
    "            \n",
    "    if len(years) == 0:\n",
    "        years.append(-1)\n",
    "        \n",
    "    n_pdfs_fetched = 0\n",
    "    \n",
    "    continue_fetching = True\n",
    "    \n",
    "    while continue_fetching:\n",
    "        \n",
    "        for year in years:\n",
    "                    \n",
    "            # -------------------------------------------------------------------\n",
    "            # SEARCH\n",
    "            # -------------------------------------------------------------------\n",
    "\n",
    "            query_parts = ['dc.format all \"application/pdf\"',]\n",
    "\n",
    "            if author != None:\n",
    "                query_parts.append('dc.title all ' + author)\n",
    "\n",
    "            if title != None:\n",
    "                query_parts.append('dc.title all ' + title)\n",
    "\n",
    "            if year != -1:\n",
    "                query_parts.append('dc.date any ' + str(year))\n",
    "\n",
    "            results = []\n",
    "\n",
    "            search_url = 'https://gallica.bnf.fr/SRU?operation=searchRetrieve&version=1.2&query=' + \\\n",
    "                            '(' + ' and '.join(query_parts) + ')' + \\\n",
    "                            '&startRecord=' + str(starting_record)\n",
    "\n",
    "            search_req = requests.get(search_url)\n",
    "\n",
    "            search_root = etree.fromstring(search_req.content)\n",
    "\n",
    "            n_records = search_root.xpath('//srw:numberOfRecords', namespaces=ns)[0].text\n",
    "            \n",
    "            # THE NEXT QUERY STARTING RECORD, IF THE SEARCH RUNS MORE THAN ONE PAGE.\n",
    "            starting_record = search_root.xpath('//srw:nextRecordPosition', namespaces=ns)[0].text\n",
    "                    \n",
    "            # -------------------------------------------------------------------\n",
    "            # FOR EVERY RECORD SEARCH FOUND\n",
    "            # -------------------------------------------------------------------\n",
    "\n",
    "            for record in search_root.xpath('//srw:record', namespaces=ns):\n",
    "\n",
    "                record_author = ''\n",
    "                if len(record.xpath('descendant::dc:creator', namespaces=ns)) > 0:\n",
    "                    record_author = record.xpath('descendant::dc:creator', namespaces=ns)[0].text\n",
    "\n",
    "                record_title = ''\n",
    "                if len(record.xpath('descendant::dc:title', namespaces=ns)) > 0:\n",
    "                    record_title = record.xpath('descendant::dc:title', namespaces=ns)[0].text\n",
    "\n",
    "                record_date = ''\n",
    "                if len(record.xpath('descendant::dc:date', namespaces=ns)) > 0:\n",
    "                    record_date = record.xpath('descendant::dc:date', namespaces=ns)[0].text\n",
    "\n",
    "                ark_id = ''\n",
    "                if len(record.xpath('descendant::uri', namespaces=ns)) > 0:\n",
    "                    ark_id = record.xpath('descendant::uri', namespaces=ns)[0].text\n",
    "                    \n",
    "                # -------------------------------------------------------------------\n",
    "                # HOW MANY PAGES?  WE NEED TO KNOW IN ORDER TO CONSTRUCT THE PDF URL\n",
    "                # -------------------------------------------------------------------\n",
    "                    \n",
    "                pagination_url = 'https://gallica.bnf.fr/services/Pagination?ark=' + ark_id\n",
    "\n",
    "                pagination_req = requests.get(pagination_url)\n",
    "\n",
    "                pagination_root = etree.fromstring(pagination_req.content)\n",
    "\n",
    "                n_images = -1\n",
    "                if len(pagination_root.xpath('//nbVueImages')) > 0:\n",
    "                    n_images = int(pagination_root.xpath('//nbVueImages')[0].text)\n",
    "                    \n",
    "                # -------------------------------------------------------------------\n",
    "                # ACTUALLY FETCH THE PDF\n",
    "                # -------------------------------------------------------------------\n",
    "        \n",
    "                pdf_url = 'https://gallica.bnf.fr/ark:/12148/' + ark_id + \\\n",
    "                            '/f1n' + str(n_images) + '.pdf?download=1'\n",
    "                \n",
    "                # I'M TRYING TO BUILD A SENSIBLE OUTPUT FILE NAME HERE\n",
    "\n",
    "                output_pdf_name = record_date + '_' + '_'.join(re.split('[ ,\\)\\)\\-\\:]+', record_author)[:2]) + \\\n",
    "                                    '_' + '_'.join(re.split('[ ,\\)\\)\\-\\:]+', record_title)[:3]) + '_' + \\\n",
    "                                    '_' + ark_id + '.pdf'\n",
    "                \n",
    "                # SOMETHING LIKE\n",
    "                #     cmd = 'wget ' + pdf_url + ' -O resulting_pdfs/' + ark_id + '.pdf'\n",
    "                # WILL PUT THE PDF'S IN THE FOLDER resulting_pdfs.\n",
    "                cmd = 'wget ' + pdf_url + ' -O ' + output_pdf_name\n",
    "\n",
    "                subprocess.getoutput(cmd)\n",
    "\n",
    "                n_pdfs_fetched += 1\n",
    "                \n",
    "            # -------------------------------------------------------------------\n",
    "            # SO IT DOESN'T GO ON FOREVER . . . \n",
    "            # -------------------------------------------------------------------\n",
    "            \n",
    "            if n_pdfs_fetched > max_n_pdfs_to_fetch:\n",
    "                continue_fetching = False\n",
    "                break\n",
    "            if int(starting_record) > int(n_records):\n",
    "                continue_fetching = False\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the routine with a particular request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pdfs_for_query(title='Toto', author='Offenbach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
