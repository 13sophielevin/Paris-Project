{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of the training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "clean_files = sorted(glob.glob('/home/seth/Notes/HDW/test/test_xml/*.xml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and tokenize the training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seth/Notes/HDW/test/test_xml/Abrantès_1844_bpt6k6472523f_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Amigues_1871_bpt6k54697084_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Asti_1843_bpt6k6471672z_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Balleydier_1849_bpt6k105490w_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Bamboches_amoureuses_1840_bpt6k1164416j_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Banville_1866_bpt6k205836j_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Bellet_1857_bpt6k6456840p_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Bonneville_1830_bpt6k5530903c_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Bréauté_1845_bpt6k64714083_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Edme_1871_bpt6k6549886k_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Fournier_1860_bpt6k6416873z_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Hugo_1831_1_bpt6k6497134z_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/LaGournerie_1854_bpt6k6394057f_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Landremont_1848_bpt6k56646183_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Lefeuve_1854_bpt6k6438988x_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Roze_1853_bpt6k5551419c_test.xml\n",
      "/home/seth/Notes/HDW/test/test_xml/Virmaître_1871_bpt6k5448441v_test.xml\n",
      "\n",
      "len(good_text) 178 len(bad_text) 895\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "def is_number(w):\n",
    "\n",
    "    result = False\n",
    "    \n",
    "    try:\n",
    "        n = int(w)\n",
    "        result = True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "def is_t_valid(t):\n",
    "    \n",
    "    terms_to_find= [r'Notre-\\s*Dame', 'Cité', 'Saint-\\s*Louis', 'Arènes', \n",
    "        r'Palais\\s*de\\s*Justice|Palais-\\s*de-\\s*Justice',\n",
    "        'Morgue', r'Sainte-\\s*Chapelle', 'Conciergerie', r'[Qq]uai\\s*de\\s*l\\'Horloge', r'Pont-\\s*Neuf', r'Cluny|Thermes',\n",
    "        r'Saint-\\s*Germain-\\s*des-\\s*Prés', 'Nesle', r'[Ss]aint-\\s*Sulpice', r'[Pp]alais\\s*du\\s* Luxembourg', \n",
    "        r'[Jj]ardin\\s*du\\s*Luxembourg', 'Observatoire', r'Panthéon|Sainte-\\s*Geneviève', r'[Eéeé]glise\\s*Saint-\\s*Étienne',\n",
    "        'Odéon', r'[Jj]ardin\\s*des\\s*Plantes', 'Gobelins', 'Auxerrois', 'Louvre', r'Carrousel|Doyenné', 'Tuileries', \n",
    "        r'Palais-\\s*Royal', r'Comédie-\\s*Française', 'Bourse', 'Innocents', 'Halles', r'Saint-\\s*Eustache', 'Temple',\n",
    "        r'[Tt]our\\s*Saint-\\s*Jacques', r'H[oôóòö]tel\\s*de\\s*Ville|Gr[eêéèë]ve', 'Rivoli', \n",
    "        r'Bastille|[Cc]olonne\\s*de\\s*Juillet', 'Tournelles', r'[Bb]oulevar[dt]\\s*de\\s*la\\s*Madeleine', 'Capucines',\n",
    "        r'[Bb]oulevard\\s*des\\s*Italiens', r'[Bb]oulevar[dt]\\s*Montmartre', r'[Bb]oulevar[dt]\\s*Poissonnière',\n",
    "        r'[Bb]oulevar[dt]\\s*Bonne-\\s*Nouvelle', r'[Bb]oulevar[dt]\\s*Saint-\\s*Denis', r'[Bb]oulevar[dt]\\s*Saint-\\s*Martin',\n",
    "        r'[Bb]oulevar[dt]\\s*du\\s*Temple|[Bb]oulevard\\s*du\\s*crime', r'[Bb]oulevar[dt]\\s*des\\s*Filles', 'Beaumarchais',\n",
    "        r'[Pp]orte\\s*Saint-Denis', r'[Cc]afé\\s*Tortoni', r'[Cc]afé\\s*Anglais', 'Maison-\\s*Dorée', \n",
    "        r'Notre-\\s*Dame-\\s*de-\\s*Lorette', 'Opéra-\\s*[Cc]omique', 'Panorama', 'Opéra', r'[Aa]venue\\s*\\s*de\\s*l’Op[eé]ra',\n",
    "        r'[Rr]ue\\s*de\\s*la\\s*Paix|[Rr]ue\\s*\\de\\s* Napoléon', 'Vivienne', r'[Rr]ue\\s*Saint-\\s*Jacques', \n",
    "        r'[Rr]ue\\s*Saint-\\s*Denis', r'[Ff]aubourg\\s*Saint-\\s*Honoré', r'[Rr]ue\\s*du\\s*[Ff]aubourg\\s*Saint-\\s*Antoine',\n",
    "        r'[Ff]aubourg\\s*Saint-\\s*Antoine', r'[Pp]lace\\s*des\\s*Vosges|Place\\s*Royale', r'Champs-\\s*Elysées', \n",
    "        r'Concorde|[Pp]lace\\s*Louis[.\\s*]XV|obélisque', r'[EÉ]toile|Triomphe', 'Vend[oôóòö]me', r'[Ll]a\\s*Madeleine', \n",
    "        'Caire', r'des\\s*Miracles', r'Quinze-\\s*Vingts', r'cimeti[eêéèë]re\\s*du\\s*P[eêéèë]re-\\s*Lachaise,' \n",
    "        r'[Bb]utte\\s*Montmartre', r'Montfaucon|[Vv]oierie', 'Chaumont', r'[Cc]h[aâáàä]teau\\s*de\\s*Vincennes', 'Invalides',\n",
    "        r'[Eéeé]cole\\s*Militaire|Champ-\\s*de-\\s*Mars', 'Grenelle']   \n",
    "    \n",
    "    result = False\n",
    "    \n",
    "    if t > '':\n",
    "        \n",
    "        was_matched = False\n",
    "        for term in terms_to_find:\n",
    "        \n",
    "            for match in re.finditer(term, t, flags=re.IGNORECASE):\n",
    "                was_matched = True\n",
    "                \n",
    "            if was_matched == True:\n",
    "                break\n",
    "        \n",
    "        if was_matched == False and is_number(t) == False:\n",
    "            result = True\n",
    "        \n",
    "    return result\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    clean_text = re.sub(r'[^\\s0123456789abcdefghijklmnopqrstuvwxyzàâäæçèéêëîïñôùûüÿœ̀]',\n",
    "                       ' ',\n",
    "                       text.lower())\n",
    "    clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "    \n",
    "    return [t for t in clean_text.lower().split(' ') if is_t_valid(t)]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "good_text = []\n",
    "bad_text = []\n",
    "\n",
    "for a in clean_files:\n",
    "    \n",
    "    print(a)\n",
    "    \n",
    "    tree = etree.parse(a)\n",
    "    \n",
    "    for node in tree.xpath('//snippet'):\n",
    "        if node.get('quality') != None and node.get('quality') == 'good':\n",
    "            good_text.append(tokenize_text(node.text))\n",
    "        else:\n",
    "            bad_text.append(tokenize_text(node.text))\n",
    "    \n",
    "print()\n",
    "print('len(good_text)', len(good_text), \n",
    "      'len(bad_text)', len(bad_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier and save it\n",
    "\n",
    "I ran this until I got a result I liked.  I'm not sure this is better than simply training the classifier on 178 good and bad examples, and not testing.  But it's what we've been doing, and I'm not comfortable with going foward with a classifer whose behavior is completely opaque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seth/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VotingClassifier ACCURACY 0.8041237113402062\n",
      "\n",
      "\t ('bad', 'bad') 642\n",
      "\t ('bad', 'good') 153\n",
      "\t ('good', 'bad') 18\n",
      "\t ('good', 'good') 60\n",
      "\n",
      "\tgood_classed_as_bad 0.23076923076923078\n",
      "\tbad_classed_as_good 0.19245283018867926\n",
      "\n",
      "classifier saved!\n",
      "\n",
      "dictionary saved!\n"
     ]
    }
   ],
   "source": [
    "import random, pickle\n",
    "from gensim import corpora, matutils\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "random.shuffle(good_text)\n",
    "random.shuffle(bad_text)\n",
    "\n",
    "training_data = [['good', text] for text in good_text[:100]] + \\\n",
    "                [['bad', text] for text in bad_text[:100]]\n",
    "\n",
    "testing_data = [['good', text] for text in good_text[100:]] + \\\n",
    "                [['bad', text] for text in bad_text[100:]]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "dictionary = corpora.Dictionary([text[1] for text in training_data] + \\\n",
    "                                [text[1] for text in testing_data])\n",
    "\n",
    "training_labels = [text[0] for text in training_data]\n",
    "training_corpus = [dictionary.doc2bow(text[1]) for text in training_data]\n",
    "\n",
    "testing_labels = [text[0] for text in testing_data]\n",
    "testing_corpus = [dictionary.doc2bow(text[1]) for text in testing_data]\n",
    "\n",
    "training_matrix = matutils.corpus2dense(training_corpus, len(dictionary))\n",
    "training_matrix = training_matrix.T\n",
    "\n",
    "testing_matrix = matutils.corpus2dense(testing_corpus, len(dictionary))\n",
    "testing_matrix = testing_matrix.T\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "estimators = [\n",
    "    ('bnb', BernoulliNB(fit_prior=False)),\n",
    "    ('rfc', RandomForestClassifier())]\n",
    "\n",
    "classifier = VotingClassifier(estimators=estimators, voting='soft')\n",
    "classifier.fit(training_matrix, training_labels)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "score = classifier.score(testing_matrix, testing_labels)\n",
    "\n",
    "print()\n",
    "print('VotingClassifier ACCURACY', score)\n",
    "\n",
    "expected_actual_result_counts = {}\n",
    "\n",
    "predictions = classifier.predict(testing_matrix)\n",
    "\n",
    "for a in range(0, len(testing_labels)):\n",
    "    if (testing_labels[a], predictions[a]) not in expected_actual_result_counts:\n",
    "        expected_actual_result_counts[(testing_labels[a], predictions[a])] = 0\n",
    "    expected_actual_result_counts[(testing_labels[a], predictions[a])] += 1\n",
    "\n",
    "print()\n",
    "for k in sorted(expected_actual_result_counts.keys()):\n",
    "    print('\\t', k, expected_actual_result_counts[k])\n",
    "\n",
    "print()\n",
    "print('\\t' 'good_classed_as_bad', float(expected_actual_result_counts[('good', 'bad')]) / \\\n",
    "         (float(expected_actual_result_counts[('good', 'bad')]) + \\\n",
    "          float(expected_actual_result_counts[('good', 'good')])))\n",
    "print('\\t' 'bad_classed_as_good', float(expected_actual_result_counts[('bad', 'good')]) / \\\n",
    "         (float(expected_actual_result_counts[('bad', 'bad')]) + \\\n",
    "          float(expected_actual_result_counts[('bad', 'good')])))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "filename = 'saved_classifier.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "print()\n",
    "print('classifier saved!')\n",
    "\n",
    "dictionary.save('classifier_dictionary.dict') \n",
    "\n",
    "print()\n",
    "print('dictionary saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
